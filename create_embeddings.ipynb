{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, alphabet = esm.pretrained.esm2_t48_15B_UR50D()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/OsmoticStress_with_binary_positions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(model, alphabet, sequence):\n",
    "    # Tokenize and prepare input\n",
    "    data = [(0, sequence)]\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[6])  # Extract representations from the last layer\n",
    "        token_embeddings = results['representations'][6]\n",
    "\n",
    "    # Average embeddings across all tokens\n",
    "    return token_embeddings.mean(dim=1).numpy()\n",
    "\n",
    "# Assuming 'model' and 'alphabet' are already loaded as shown earlier\n",
    "df['Full_embedding'] = df['full_sequence'][:2].apply(lambda x: generate_embeddings(model, alphabet, x))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
