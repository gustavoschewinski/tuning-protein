{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model\n",
    "We are aiming to predict the Lip_norm parameter based on the binary positions and full embedding of a peptide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and prepare embeddings\n",
    "Read generated embeddings from the pkl file, that was generated in notebook 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/dataset_1.pkl\n",
      "Processing segment 1 from data/dataset_1.pkl\n",
      "Processing data/dataset_2.pkl\n",
      "Processing segment 1 from data/dataset_2.pkl\n",
      "Processing data/dataset_3.pkl\n",
      "Processing segment 1 from data/dataset_3.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_and_process_embedding(file_path):\n",
    "    # Function to process data from file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        try:\n",
    "            segment_count = 0  # Initialize segment counter\n",
    "            while True:\n",
    "                # Load each segment of data\n",
    "                data = pickle.load(file)\n",
    "                embeddings = pd.DataFrame(data['embeddings'])\n",
    "                binary_positions = pd.DataFrame(data['Padded_Binary_Positions'])\n",
    "                y_values = pd.DataFrame(data['Log2FC(LiP_norm)'])\n",
    "\n",
    "                segment_count += 1  # Increment segment counter\n",
    "                print(f\"Processing segment {segment_count} from {file_path}\")\n",
    "\n",
    "                yield (embeddings, binary_positions, y_values)\n",
    "        except EOFError:\n",
    "            # End of file reached\n",
    "            pass\n",
    "\n",
    "embedding_files = [\n",
    "    'data/dataset_1.pkl',\n",
    "    'data/dataset_2.pkl',\n",
    "    'data/dataset_3.pkl',\n",
    "]\n",
    "\n",
    "embeddings_tensors = []\n",
    "binary_positions_tensors = []\n",
    "y_tensors = []\n",
    "\n",
    "for file_path in embedding_files:\n",
    "    print(f\"Processing {file_path}\")\n",
    "    for embeddings_segment, binary_positions_segment, y_segment in load_and_process_embedding(file_path):\n",
    "        # Convert DataFrames to tensors and send to device\n",
    "        embeddings_tensor = torch.tensor(np.array(embeddings_segment.values.tolist(), dtype=np.float16).reshape(-1, 1000, 1280), device=device)\n",
    "        binary_positions_tensor = torch.tensor(np.array(binary_positions_segment.values.tolist(), dtype=np.float16).reshape(-1, 1000), device=device)\n",
    "        y_tensor = torch.tensor(np.array(y_segment.values.squeeze().tolist(), dtype=np.float16), device=device)\n",
    "\n",
    "        embeddings_tensors.append(embeddings_tensor)\n",
    "        binary_positions_tensors.append(binary_positions_tensor)\n",
    "        y_tensors.append(y_tensor)\n",
    "\n",
    "final_embeddings_tensor = torch.cat(embeddings_tensors, dim=0)\n",
    "final_binary_positions_tensor = torch.cat(binary_positions_tensors, dim=0)\n",
    "final_y_tensor = torch.cat(y_tensors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4818, 1000, 1280])\n",
      "torch.Size([4818, 1000])\n",
      "torch.Size([4818])\n"
     ]
    }
   ],
   "source": [
    "print(final_embeddings_tensor.shape)\n",
    "print(final_binary_positions_tensor.shape)\n",
    "print(final_y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4818])\n"
     ]
    }
   ],
   "source": [
    "# Scaling data [0,1]\n",
    "\n",
    "min_y = final_y_tensor.min()\n",
    "max_y = final_y_tensor.max()\n",
    "\n",
    "final_y_tensor = (final_y_tensor - min_y) / (max_y - min_y)\n",
    "\n",
    "print(final_y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4818, 1000, 1280])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4818.000000\n",
      "mean        0.138916\n",
      "std         0.065613\n",
      "min        -1.000000\n",
      "25%         0.101562\n",
      "50%         0.143555\n",
      "75%         0.173828\n",
      "max         1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y_series = pd.Series(final_y_tensor.cpu())\n",
    "\n",
    "# Get a statistical summary\n",
    "summary = y_series.describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Now that we have both input and target data, we can create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/nasif12/home_if12/l_schewinski/.conda/envs/l-schewinski-tp-2/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "/data/nasif12/home_if12/l_schewinski/.conda/envs/l-schewinski-tp-2/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/data/nasif12/home_if12/l_schewinski/.conda/envs/l-schewinski-tp-2/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(16)\n",
    "\n",
    "# Data setup\n",
    "class PeptideDataset(Dataset):\n",
    "    def __init__(self, embeddings, position, y):\n",
    "        self.embeddings = torch.tensor(embeddings, dtype = torch.float16)\n",
    "        self.position = torch.tensor(position, dtype = torch.int64)\n",
    "        self.y = torch.tensor(y, dtype = torch.float16)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = {\"embeddings\":self.embeddings[index],\n",
    "             \"position\":self.position[index]}\n",
    "        y = self.y[index]\n",
    "        return x, y\n",
    "\n",
    "############################################# If shuffle true ##############################################\n",
    "dataset = PeptideDataset(final_embeddings_tensor, final_binary_positions_tensor, final_y_tensor)\n",
    "\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = int(0.3 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size,test_size])\n",
    "\n",
    "\n",
    "# final_y_np = final_y_tensor.cpu().numpy()\n",
    "\n",
    "# # Determine the number of bins\n",
    "# num_bins = np.floor(1 + 3.322 * np.log10(len(final_y_np)))  # Using Sturges' rule\n",
    "\n",
    "# # Bin indices for each element in the dataset\n",
    "# bins = np.digitize(final_y_np, np.histogram_bin_edges(final_y_np, bins=int(num_bins)))\n",
    "\n",
    "# # Custom dataset split function\n",
    "# def stratified_split(dataset, bins, train_ratio=0.5, val_ratio=0.3):\n",
    "#     from torch.utils.data import Subset\n",
    "#     indices = np.arange(len(dataset))\n",
    "#     train_indices, test_indices, val_indices = [], [], []\n",
    "\n",
    "#     # For each bin, allocate appropriately to train, val, and test\n",
    "#     for b in np.unique(bins):\n",
    "#         b_indices = indices[bins == b]\n",
    "#         np.random.shuffle(b_indices)\n",
    "#         train_end = int(len(b_indices) * train_ratio)\n",
    "#         val_end = train_end + int(len(b_indices) * val_ratio)\n",
    "        \n",
    "#         train_indices.extend(b_indices[:train_end])\n",
    "#         val_indices.extend(b_indices[train_end:val_end])\n",
    "#         test_indices.extend(b_indices[val_end:])\n",
    "\n",
    "#     # Create subsets\n",
    "#     train_dataset = Subset(dataset, train_indices)\n",
    "#     val_dataset = Subset(dataset, val_indices)\n",
    "#     test_dataset = Subset(dataset, test_indices)\n",
    "#     return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# # Apply stratified split\n",
    "# train_dataset, val_dataset, test_dataset = stratified_split(dataset, bins)\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# train_dataset = PeptideDataset(train_embeddings, train_positions, train_y)\n",
    "# val_dataset = PeptideDataset(val_embeddings, val_positions, val_y)\n",
    "# test_dataset = PeptideDataset(test_embeddings, test_positions, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model definition\n",
    "class PeptideRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dropout_rate):\n",
    "        super(PeptideRegressor, self).__init__()\n",
    "\n",
    "        self.fc_embed = torch.nn.Linear(1280, 256)\n",
    "        \n",
    "        self.seq_pos_embed = nn.Embedding(1000, 1280)\n",
    "        self.pept_pos_embed = nn.Embedding(2, 1280)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=256, \n",
    "            nhead=num_heads, \n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        positions = x[\"position\"]\n",
    "        x = x[\"embeddings\"] + self.seq_pos_embed(torch.arange(x[\"position\"].size(1), device=device)) + self.pept_pos_embed(x[\"position\"])\n",
    "        x = x.permute(1, 0, 2)  # Permute to [sequence_length, batch_size, features]\n",
    "\n",
    "        embed = self.fc_embed(x)\n",
    "        transformer_output = self.transformer_encoder(embed)\n",
    "        transformer_output = transformer_output.permute(1, 2, 0)\n",
    "        \n",
    "        mask = positions == 1\n",
    "        mask = mask.unsqueeze(1)\n",
    "        peptide_embeddings = transformer_output * mask.float()\n",
    "        pooled = torch.sum(peptide_embeddings, dim=-1)/torch.sum(positions==1,dim=-1).unsqueeze(1)\n",
    "        \n",
    "        output = self.fc(pooled)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.086589515209198\n",
      "Epoch 2, Train Loss: 51.4577751159668\n",
      "Epoch 3, Train Loss: 15.127979278564453\n",
      "Epoch 4, Train Loss: 0.6316688656806946\n",
      "Epoch 5, Train Loss: 1.3694283962249756\n",
      "Epoch 6, Train Loss: 2.6347503662109375\n",
      "Epoch 7, Train Loss: 1.6915113925933838\n",
      "Epoch 8, Train Loss: 0.34772342443466187\n",
      "Epoch 9, Train Loss: 0.06529161334037781\n",
      "Epoch 10, Train Loss: 0.5959973931312561\n",
      "Epoch 11, Train Loss: 0.713252067565918\n",
      "Epoch 12, Train Loss: 0.40844079852104187\n",
      "Epoch 13, Train Loss: 0.1089600920677185\n",
      "Epoch 14, Train Loss: 0.003636422334238887\n",
      "Epoch 15, Train Loss: 0.06186347454786301\n",
      "Epoch 16, Train Loss: 0.16777734458446503\n",
      "Epoch 17, Train Loss: 0.23436564207077026\n",
      "Epoch 18, Train Loss: 0.2244412750005722\n",
      "Epoch 19, Train Loss: 0.15727639198303223\n",
      "Epoch 20, Train Loss: 0.08098042011260986\n",
      "Epoch 21, Train Loss: 0.02731305919587612\n",
      "Epoch 22, Train Loss: 0.0037010610103607178\n",
      "Epoch 23, Train Loss: 0.0118635855615139\n",
      "Epoch 24, Train Loss: 0.036129482090473175\n",
      "Epoch 25, Train Loss: 0.06270995736122131\n",
      "Epoch 26, Train Loss: 0.07212045043706894\n",
      "Epoch 27, Train Loss: 0.06872113794088364\n",
      "Epoch 28, Train Loss: 0.05330575257539749\n",
      "Epoch 29, Train Loss: 0.030432745814323425\n",
      "Epoch 30, Train Loss: 0.012372340075671673\n",
      "Epoch 31, Train Loss: 0.00448308140039444\n",
      "Epoch 32, Train Loss: 0.003557723481208086\n",
      "Epoch 33, Train Loss: 0.009941933676600456\n",
      "Epoch 34, Train Loss: 0.01838000677525997\n",
      "Epoch 35, Train Loss: 0.022816812619566917\n",
      "Epoch 36, Train Loss: 0.026387428864836693\n",
      "Epoch 37, Train Loss: 0.023300420492887497\n",
      "Epoch 38, Train Loss: 0.0181733351200819\n",
      "Epoch 39, Train Loss: 0.009765802882611752\n",
      "Epoch 40, Train Loss: 0.004704848397523165\n",
      "Epoch 41, Train Loss: 0.002989180153235793\n",
      "Epoch 42, Train Loss: 0.004346303641796112\n",
      "Epoch 43, Train Loss: 0.0072215767577290535\n",
      "Epoch 44, Train Loss: 0.01009828969836235\n",
      "Epoch 45, Train Loss: 0.011841198429465294\n",
      "Epoch 46, Train Loss: 0.010727308690547943\n",
      "Epoch 47, Train Loss: 0.008255470544099808\n",
      "Epoch 48, Train Loss: 0.0064317043870687485\n",
      "Epoch 49, Train Loss: 0.003330725710839033\n",
      "Epoch 50, Train Loss: 0.0031091473065316677\n",
      "Epoch 51, Train Loss: 0.003401879919692874\n",
      "Epoch 52, Train Loss: 0.0036884862929582596\n",
      "Epoch 53, Train Loss: 0.004964322317391634\n",
      "Epoch 54, Train Loss: 0.005261573940515518\n",
      "Epoch 55, Train Loss: 0.005136705469340086\n",
      "Epoch 56, Train Loss: 0.0059003280475735664\n",
      "Epoch 57, Train Loss: 0.004282456822693348\n",
      "Epoch 58, Train Loss: 0.003803080413490534\n",
      "Epoch 59, Train Loss: 0.0027623935602605343\n",
      "Epoch 60, Train Loss: 0.0028917789459228516\n",
      "Epoch 61, Train Loss: 0.0035923756659030914\n",
      "Epoch 62, Train Loss: 0.003436808241531253\n",
      "Epoch 63, Train Loss: 0.0038061304949223995\n",
      "Epoch 64, Train Loss: 0.0034310154151171446\n",
      "Epoch 65, Train Loss: 0.003252549795433879\n",
      "Epoch 66, Train Loss: 0.003308255225419998\n",
      "Epoch 67, Train Loss: 0.0029806047677993774\n",
      "Epoch 68, Train Loss: 0.002387072192505002\n",
      "Epoch 69, Train Loss: 0.0030424080323427916\n",
      "Epoch 70, Train Loss: 0.003425692208111286\n",
      "Epoch 71, Train Loss: 0.0030537410639226437\n",
      "Epoch 72, Train Loss: 0.003021701006218791\n",
      "Epoch 73, Train Loss: 0.002874927129596472\n",
      "Epoch 74, Train Loss: 0.0027004368603229523\n",
      "Epoch 75, Train Loss: 0.002603896427899599\n",
      "Epoch 76, Train Loss: 0.0026892530731856823\n",
      "Epoch 77, Train Loss: 0.0032271023374050856\n",
      "Epoch 78, Train Loss: 0.0029961662366986275\n",
      "Epoch 79, Train Loss: 0.0030473123770207167\n",
      "Epoch 80, Train Loss: 0.00337801082059741\n",
      "Epoch 81, Train Loss: 0.002781826537102461\n",
      "Epoch 82, Train Loss: 0.003044710261747241\n",
      "Epoch 83, Train Loss: 0.002896669087931514\n",
      "Epoch 84, Train Loss: 0.002777205780148506\n",
      "Epoch 85, Train Loss: 0.0034220372326672077\n",
      "Epoch 86, Train Loss: 0.0026303313206881285\n",
      "Epoch 87, Train Loss: 0.0030766609124839306\n",
      "Epoch 88, Train Loss: 0.002831216435879469\n",
      "Epoch 89, Train Loss: 0.002835450693964958\n",
      "Epoch 90, Train Loss: 0.002728638704866171\n",
      "Epoch 91, Train Loss: 0.003038655035197735\n",
      "Epoch 92, Train Loss: 0.0025735939852893353\n",
      "Epoch 93, Train Loss: 0.0028856336139142513\n",
      "Epoch 94, Train Loss: 0.0029700356535613537\n",
      "Epoch 95, Train Loss: 0.0031013290863484144\n",
      "Epoch 96, Train Loss: 0.0032474915497004986\n",
      "Epoch 97, Train Loss: 0.0033002663403749466\n",
      "Epoch 98, Train Loss: 0.0027812018524855375\n",
      "Epoch 99, Train Loss: 0.003067451063543558\n",
      "Epoch 100, Train Loss: 0.0033898993860930204\n",
      "Epoch 101, Train Loss: 0.002751218155026436\n",
      "Epoch 102, Train Loss: 0.0023675025440752506\n",
      "Epoch 103, Train Loss: 0.002269672229886055\n",
      "Epoch 104, Train Loss: 0.002581375651061535\n",
      "Epoch 105, Train Loss: 0.003275662660598755\n",
      "Epoch 106, Train Loss: 0.002863452536985278\n",
      "Epoch 107, Train Loss: 0.002667082939296961\n",
      "Epoch 108, Train Loss: 0.003320103045552969\n",
      "Epoch 109, Train Loss: 0.003135757287964225\n",
      "Epoch 110, Train Loss: 0.0030497703701257706\n",
      "Epoch 111, Train Loss: 0.002778224181383848\n",
      "Epoch 112, Train Loss: 0.0029597727116197348\n",
      "Epoch 113, Train Loss: 0.0032214883249253035\n",
      "Epoch 114, Train Loss: 0.0027930184733122587\n",
      "Epoch 115, Train Loss: 0.0028883274644613266\n",
      "Epoch 116, Train Loss: 0.0027840021066367626\n",
      "Epoch 117, Train Loss: 0.002892717719078064\n",
      "Epoch 118, Train Loss: 0.0028978660702705383\n",
      "Epoch 119, Train Loss: 0.0026108461897820234\n",
      "Epoch 120, Train Loss: 0.0026779561303555965\n",
      "Epoch 121, Train Loss: 0.002920933533459902\n",
      "Epoch 122, Train Loss: 0.0034613802563399076\n",
      "Epoch 123, Train Loss: 0.002862404566258192\n",
      "Epoch 124, Train Loss: 0.0032764622010290623\n",
      "Epoch 125, Train Loss: 0.0031131142750382423\n",
      "Epoch 126, Train Loss: 0.0031465336214751005\n",
      "Epoch 127, Train Loss: 0.0032979294192045927\n",
      "Epoch 128, Train Loss: 0.0028894315473735332\n",
      "Epoch 129, Train Loss: 0.003394653555005789\n",
      "Epoch 130, Train Loss: 0.002412762027233839\n",
      "Epoch 131, Train Loss: 0.002719912678003311\n",
      "Epoch 132, Train Loss: 0.002489633858203888\n",
      "Epoch 133, Train Loss: 0.0033509377390146255\n",
      "Epoch 134, Train Loss: 0.0032055904157459736\n",
      "Epoch 135, Train Loss: 0.0027286529075354338\n",
      "Epoch 136, Train Loss: 0.0026906111743301153\n",
      "Epoch 137, Train Loss: 0.0030286952387541533\n",
      "Epoch 138, Train Loss: 0.0027640690095722675\n",
      "Epoch 139, Train Loss: 0.0025105630047619343\n",
      "Epoch 140, Train Loss: 0.003085228381678462\n",
      "Epoch 141, Train Loss: 0.0023157449904829264\n",
      "Epoch 142, Train Loss: 0.0022613282781094313\n",
      "Epoch 143, Train Loss: 0.002487707184627652\n",
      "Epoch 144, Train Loss: 0.0028287849854677916\n",
      "Epoch 145, Train Loss: 0.0030791983008384705\n",
      "Epoch 146, Train Loss: 0.002950902096927166\n",
      "Epoch 147, Train Loss: 0.0028615230694413185\n",
      "Epoch 148, Train Loss: 0.002624498214572668\n",
      "Epoch 149, Train Loss: 0.002943922532722354\n",
      "Epoch 150, Train Loss: 0.0029140482656657696\n",
      "Epoch 151, Train Loss: 0.0029137865640223026\n",
      "Epoch 152, Train Loss: 0.002373169641941786\n",
      "Epoch 153, Train Loss: 0.0026446485426276922\n",
      "Epoch 154, Train Loss: 0.002852944191545248\n",
      "Epoch 155, Train Loss: 0.003419195767492056\n",
      "Epoch 156, Train Loss: 0.0025571375153958797\n",
      "Epoch 157, Train Loss: 0.0025593433529138565\n",
      "Epoch 158, Train Loss: 0.002781769260764122\n",
      "Epoch 159, Train Loss: 0.0029623545706272125\n",
      "Epoch 160, Train Loss: 0.0027135552372783422\n",
      "Epoch 161, Train Loss: 0.0027774679474532604\n",
      "Epoch 162, Train Loss: 0.002590667922049761\n",
      "Epoch 163, Train Loss: 0.002613753080368042\n",
      "Epoch 164, Train Loss: 0.0024954495020210743\n",
      "Epoch 165, Train Loss: 0.0027990543749183416\n",
      "Epoch 166, Train Loss: 0.003361426293849945\n",
      "Epoch 167, Train Loss: 0.002812154358252883\n",
      "Epoch 168, Train Loss: 0.003533880924805999\n",
      "Epoch 169, Train Loss: 0.002550759119912982\n",
      "Epoch 170, Train Loss: 0.0029782233759760857\n",
      "Epoch 171, Train Loss: 0.0027327470015734434\n",
      "Epoch 172, Train Loss: 0.0031929458491504192\n",
      "Epoch 173, Train Loss: 0.0034678454976528883\n",
      "Epoch 174, Train Loss: 0.0029446431435644627\n",
      "Epoch 175, Train Loss: 0.0027517955750226974\n",
      "Epoch 176, Train Loss: 0.0031217995565384626\n",
      "Epoch 177, Train Loss: 0.0025718798860907555\n",
      "Epoch 178, Train Loss: 0.0031926685478538275\n",
      "Epoch 179, Train Loss: 0.0026104855351150036\n",
      "Epoch 180, Train Loss: 0.003149281023070216\n",
      "Epoch 181, Train Loss: 0.003356918226927519\n",
      "Epoch 182, Train Loss: 0.002842068439349532\n",
      "Epoch 183, Train Loss: 0.0032708747312426567\n",
      "Epoch 184, Train Loss: 0.002911727409809828\n",
      "Epoch 185, Train Loss: 0.0030890393536537886\n",
      "Epoch 186, Train Loss: 0.002488669939339161\n",
      "Epoch 187, Train Loss: 0.002176787005737424\n",
      "Epoch 188, Train Loss: 0.002897020895034075\n",
      "Epoch 189, Train Loss: 0.00328647973947227\n",
      "Epoch 190, Train Loss: 0.0027573739644140005\n",
      "Epoch 191, Train Loss: 0.0029011829756200314\n",
      "Epoch 192, Train Loss: 0.002905540866777301\n",
      "Epoch 193, Train Loss: 0.0029202408622950315\n",
      "Epoch 194, Train Loss: 0.0024637700989842415\n",
      "Epoch 195, Train Loss: 0.0027351616881787777\n",
      "Epoch 196, Train Loss: 0.0029282825998961926\n",
      "Epoch 197, Train Loss: 0.0029412521980702877\n",
      "Epoch 198, Train Loss: 0.00261578056961298\n",
      "Epoch 199, Train Loss: 0.002453923225402832\n",
      "Epoch 200, Train Loss: 0.002659441903233528\n",
      "[0.1334, 0.1334, 0.1334, 0.1334, 0.1334, 0.1334, 0.1334, 0.1334, 0.1334, 0.1334]\n",
      "[0.1327, 0.1302, 0.1362, 0.1332, 0.1385, 0.1345, 0.1287, 0.1326, 0.13, 0.1376]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Overfit to 1 batch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = PeptideRegressor(input_dim=1280, num_heads=4, dropout_rate=0.05)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training settings\n",
    "n_epochs = 200\n",
    "best_val_loss = float('inf')\n",
    "train_losses_avg = []\n",
    "val_losses_avg = []\n",
    "\n",
    "single_batch_X, single_batch_y = next(iter(train_loader))\n",
    "for k, v in single_batch_X.items():\n",
    "    single_batch_X[k] = v.to(device, non_blocking=True)\n",
    "single_batch_y = single_batch_y.to(device, non_blocking=True)\n",
    "\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "# Training loop using a single batch for overfitting check\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with autocast():\n",
    "        y_pred = model(single_batch_X)\n",
    "        loss = loss_fn(y_pred, single_batch_y.unsqueeze(1))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    train_loss_avg = sum(train_losses) / len(train_losses)\n",
    "\n",
    "    mean_pred = y_pred.detach().cpu().numpy().mean()\n",
    "    mean_actual = single_batch_y.detach().cpu().numpy().mean()\n",
    "    predictions.append(mean_pred)\n",
    "    actuals.append(mean_actual)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss_avg}')\n",
    "\n",
    "predictions_plot = predictions[-10:]\n",
    "actuals_plot = actuals[-10:]\n",
    "\n",
    "print(actuals_plot)\n",
    "print(predictions_plot)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(predictions_plot, actuals_plot, alpha=0.5, color='blue')\n",
    "ax.set_xlabel('Predicted Values')\n",
    "ax.set_ylabel('Actual Values')\n",
    "ax.set_title('Predicted vs Actual Values')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.030682886590842496\n",
      "Epoch 1, Val MSE: 0.007245230950323529\n",
      "Epoch 2, Train Loss: 0.006860323483124375\n",
      "Epoch 2, Val MSE: 0.006410458850384247\n",
      "Epoch 3, Train Loss: 0.006193303829251427\n",
      "Epoch 3, Val MSE: 0.006266547454765342\n",
      "Epoch 4, Train Loss: 0.007159947245458742\n",
      "Epoch 4, Val MSE: 0.006142460828891299\n",
      "Epoch 5, Train Loss: 0.006126276595510296\n",
      "Epoch 5, Val MSE: 0.007012336205721634\n",
      "Epoch 6, Train Loss: 0.005887865935536948\n",
      "Epoch 6, Val MSE: 0.012429048948478504\n",
      "Epoch 7, Train Loss: 0.005314583884367396\n",
      "Epoch 7, Val MSE: 0.005540549831434352\n",
      "Epoch 8, Train Loss: 0.0053502400347978365\n",
      "Epoch 8, Val MSE: 0.005417073353529587\n",
      "Epoch 9, Train Loss: 0.005416575204465569\n",
      "Epoch 9, Val MSE: 0.00589408944208236\n",
      "Epoch 10, Train Loss: 0.005023990374843567\n",
      "Epoch 10, Val MSE: 0.005513330814657641\n",
      "Epoch 11, Train Loss: 0.0049285966102495665\n",
      "Epoch 11, Val MSE: 0.00630746501475023\n",
      "Epoch 12, Train Loss: 0.004792001952267106\n",
      "Epoch 12, Val MSE: 0.005121703572632348\n",
      "Epoch 13, Train Loss: 0.00434863981636903\n",
      "Epoch 13, Val MSE: 0.006925342860249955\n",
      "Epoch 14, Train Loss: 0.005175657471264751\n",
      "Epoch 14, Val MSE: 0.005810665272054125\n",
      "Epoch 15, Train Loss: 0.004803335196954344\n",
      "Epoch 15, Val MSE: 0.0068854667307412035\n",
      "Epoch 16, Train Loss: 0.004356752049701273\n",
      "Epoch 16, Val MSE: 0.005928099290925826\n",
      "Epoch 17, Train Loss: 0.005453901789689914\n",
      "Epoch 17, Val MSE: 0.010077105438122983\n",
      "Epoch 18, Train Loss: 0.0041469512300465776\n",
      "Epoch 18, Val MSE: 0.006503726138168427\n",
      "Epoch 19, Train Loss: 0.0046018406802649835\n",
      "Epoch 19, Val MSE: 0.005107010483310451\n",
      "Epoch 20, Train Loss: 0.0045310216340090125\n",
      "Epoch 20, Val MSE: 0.00502715778411137\n",
      "Epoch 21, Train Loss: 0.004533826496667606\n",
      "Epoch 21, Val MSE: 0.0062010862223315436\n",
      "Epoch 22, Train Loss: 0.0038311836768657524\n",
      "Epoch 22, Val MSE: 0.005034436269464796\n",
      "Epoch 23, Train Loss: 0.00404446105634378\n",
      "Epoch 23, Val MSE: 0.005255176821250285\n",
      "Epoch 24, Train Loss: 0.004192444189905751\n",
      "Epoch 24, Val MSE: 0.00576231027686144\n",
      "Epoch 25, Train Loss: 0.004130291257030523\n",
      "Epoch 25, Val MSE: 0.005025216361095548\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/tmp/l_schewinski/ipykernel_346831/2035251291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/l-schewinski-tp-2/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/l-schewinski-tp-2/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "model = PeptideRegressor(input_dim=1280, num_heads=8, dropout_rate=0.05)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "# loss_fn = nn.HuberLoss(delta=1.5)\n",
    "# loss_fn = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=1e-2)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.0005)000005\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training settings\n",
    "n_epochs = 30\n",
    "best_val_loss = float('inf')\n",
    "train_losses_avg = []\n",
    "val_losses_avg = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        for k, v in X_batch.items():\n",
    "            X_batch[k] = v.to(device, non_blocking=True)\n",
    "        y_batch = y_batch.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_losses_avg.append(sum(train_losses) / len(train_losses))\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_losses_avg[-1]}')\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        for k, v in X_batch.items():\n",
    "            X_batch[k] = v.to(device, non_blocking=True)\n",
    "        y_batch = y_batch.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.no_grad(), autocast():\n",
    "            y_pred = model(X_batch)\n",
    "            mse = loss_fn(y_pred, y_batch.unsqueeze(1))\n",
    "            val_losses.append(mse.item())\n",
    "\n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    val_losses_avg.append(avg_val_loss)\n",
    "    print(f'Epoch {epoch+1}, Val MSE: {avg_val_loss}')\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# torch.save({\n",
    "#     'model_state_dict': best_model,\n",
    "# }, 'best_peptide_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the training process\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses_avg[1:], label='Train Loss')\n",
    "plt.plot(val_losses_avg[1:], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model):\n",
    "    print(\"Model Summary:\")\n",
    "    print(\"{:<50} {:<30} {:<15} {:<15}\".format(\"Layer Name\", \"Shape\", \"Parameters\", \"Trainable\"))\n",
    "    print(\"-\" * 110)\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "    lm_params = 0\n",
    "    lm_trainable_params = 0\n",
    "    lm_layers = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        param = parameter.numel()\n",
    "        total_params += param\n",
    "        # Check if the parameter is trainable\n",
    "        trainable = parameter.requires_grad\n",
    "        trainable_param = param if trainable else 0\n",
    "        total_trainable_params += trainable_param\n",
    "        print(\"{:<50} {:<30} {:<15} {:<15}\".format(name, str(parameter.size()), param, trainable_param))\n",
    "    print(\"-\" * 110)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {total_trainable_params}\")\n",
    "\n",
    "model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "test_losses = []\n",
    "y_preds = []\n",
    "y_actuals = []\n",
    "\n",
    "for X_batch, y_batch in test_loader:\n",
    "    for k, v in X_batch.items():\n",
    "        X_batch[k] = v.to(device, non_blocking=True)\n",
    "    y_batch = y_batch.to(device, non_blocking=True)\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        y_pred = model(X_batch)\n",
    "        mse = loss_fn(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        y_preds.extend(y_pred.cpu().numpy())\n",
    "        y_actuals.extend(y_batch.cpu().numpy())\n",
    "        test_losses.append(mse.item())\n",
    "\n",
    "avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "print(f'Test MSE: {avg_test_loss}')\n",
    "\n",
    "MSE_original = avg_test_loss * (max_y - min_y)**2\n",
    "\n",
    "print(max_y, min_y)\n",
    "print(\"The MSE on the original scale is:\", MSE_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_actuals, y_preds, alpha=0.5)\n",
    "# plt.plot([min(y_actuals), max(y_actuals)], [min(y_actuals), max(y_actuals)], 'r')  # Diagonal line\n",
    "plt.title('Predicted vs. Actual Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "spearman_corr, p_value = spearmanr(y_actuals, y_preds)\n",
    "\n",
    "# Print the Spearman correlation coefficient and p-value\n",
    "print(f'Spearman Correlation: {spearman_corr}')\n",
    "print(f'P-value: {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-l-schewinski-tp-2]",
   "language": "python",
   "name": "conda-env-.conda-l-schewinski-tp-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
